{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Review Analysis\n",
    "\n",
    "Data source : https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
    "Contents\n",
    "\n",
    "    Reviews.csv: Pulled from the corresponding SQLite table named Reviews in database.sqlite\n",
    "    database.sqlite: Contains the table 'Reviews'\n",
    "\n",
    "Data includes:\n",
    "\n",
    "    Reviews from \n",
    "    Number of reviews : 568,454\n",
    "    Number of Users : 256,059 users\n",
    "    Number of Products : 74,258 products\n",
    "    Timespan : Oct 1999 - Oct 2012\n",
    "    Number of Attributes/Columns : 10\n",
    "    260 users with > 50 reviews\n",
    "    \n",
    "Attribute Information :\n",
    "\n",
    "    1. Id\n",
    "    2. ProductId : Unique identifier for the product\n",
    "    3. UserId : Unqiue identifier for the user\n",
    "    4. ProfileName : Profile name of the user\n",
    "    5. HelpfulnessNumerator : Number of users who found the review helpful\n",
    "    6. HelpfulnessDenominator : Number of users who indicated whether they found the review helpful or not\n",
    "    7. Score : Rating between 1 and 5\n",
    "    8. Time : Timestamp for the review\n",
    "    9. Summary : Brief summary of the review\n",
    "    10. Text : Text of the review\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset \n",
    "After Downloading the data we can see that the data is in sqlite format which will help us to make query and visualisation simple and easy.\n",
    "So first Since here we have 10 attribute. So for analysis and prediction purpose we are creating another attribute in place of score attribute by simply classifying the positive scores (Score 4 and score 5) and Negative Scores (Score 1 and score 2) and will ignore the score level of 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect=sqlite3.connect('database.sqlite')\n",
    "\n",
    "#Filtering only positive and negative reviews i.e. just avoiding taking reviews having score =3\n",
    "filtered_data=pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score!=3\"\"\",connect)\n",
    "\n",
    "#for filteration as positive and negative \n",
    "\n",
    "def partition(x):\n",
    "    if x>3:\n",
    "        return 'positive'\n",
    "    return 'negative'\n",
    "\n",
    "#now putting change into the table\n",
    "actual_score=filtered_data['Score']\n",
    "positivenegative=actual_score.map(partition)\n",
    "filtered_data['Score']=positivenegative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratary Data Analysis\n",
    "\n",
    "#### Data Cleaning : Duplication\n",
    "As we have seen the data, this data contains many duplicate entries. As many discription of this data set we have come to know that this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_data=pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score!=3 AND UserId='AR5J8UI46CURR' ORDER BY ProductID\"\"\", connect);\n",
    "display_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting the data according to ProductID in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId',axis=0,ascending=True)\n",
    "sorted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deduplication of entries\n",
    "final_data=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep=\"first\",inplace=False)\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing some errored data\n",
    "final_data=final_data[final_data.HelpfulnessNumerator<=final_data.HelpfulnessDenominator]\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now just to Count how many positive and negative reviews are present\n",
    "final_data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "As we can see through the data the most important attribute is text attributes i.e. Summery and Text. So if we are able to convert anything to vector then we can perform linear algebra stuff with the data. \n",
    "* So if we are able to convert the text part into the n dimension vector then we can perform many linear algebra things with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Word (BoW):\n",
    "    1: Constructing a dictionary : Constructing set of all the words\n",
    "    2: Constructing a matrix which represent each of the word of the dictionary. \\\n",
    "        -> Constructing a vector of size of the dictinary (say d) where each word is a different dimension. \n",
    "        -> Where each cell of the vector represent the occurance of that particular word.\n",
    "        -> So we can generate a vector of size n (dictionary sized) for each review where it can be sparse matrix.(most of its values will be  zero)\n",
    " \n",
    "    With the help of BoW, we can able to construct some kind of relation between each of the reviews. Similiar text will result closer vector in d dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag-of-Words\n",
    "count_vect=CountVectorizer() #in scikit-learn\n",
    "final_count=count_vect.fit_transform(final_data['Text'].values)\n",
    "#print(final_count)\n",
    "#final_count is the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(364171, 115281)\n"
     ]
    }
   ],
   "source": [
    "print(type(final_count))\n",
    "print(final_count.shape)\n",
    "\n",
    "\n",
    "#print(scipy.sparse.csr_matrix.todense(final_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that it converts to values such that there are 115281 columns each representing a distinct word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming, Stop Words removing and lemmatization\n",
    "* Removing the html tags\n",
    "* Removing pancuations or special charector like or, #...\n",
    "* check if the word is made up of english language or alphanumeric\n",
    "* check to see if length of the word greater then 2.\n",
    "* convert the words to lower case\n",
    "* remove stop words\n",
    "* Final Snowball stemming the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "#print(stop)\n",
    "sno=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#clean the data tags\n",
    "def clean_tags(data):\n",
    "    clean_val=re.compile('<.*?>')\n",
    "    clean_text=re.sub(clean_val, ' ', data)\n",
    "    return clean_text\n",
    "def clean_panctuations(data):\n",
    "    cleaned=re.sub(r'[?|!|\\'|\"|#]',r'',data)\n",
    "    cleaned=re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return cleaned\n",
    "#Stemming\n",
    "print(sno.stem(\"Greetings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((final_data['Score'].values[900]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "str1=''\n",
    "final_string=[]\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "s=''\n",
    "#'''''''\n",
    "for sent in final_data['Text'].values:\n",
    "    filtered_sentences=[]\n",
    "    sent=clean_tags(sent)\n",
    "    \n",
    "    \n",
    "    for w in sent.split():\n",
    "        for cleaned_words in clean_panctuations(w).split():\n",
    "            if ((cleaned_words.isalpha())& (len(cleaned_words)>2)):\n",
    "                if (cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower()).encode('utf8'))\n",
    "                    filtered_sentences.append(s)\n",
    "                    if (final_data['Score'].values)[i]=='positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if (final_data['Score'].values)[i]=='negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    #final string after cleaning\n",
    "    string1= b\" \".join(filtered_sentences)\n",
    "    final_string.append(string1)\n",
    "    i=i+1\n",
    "                \n",
    "#print(final_string)                \n",
    "               \n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['cleanned_text']=final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>b'witti littl book make son laugh loud recit c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId         UserId      ProfileName  \\\n",
       "138706  150524  0006641040  ACITT7DI6IDDL  shari zychinski   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score       Time  \\\n",
       "138706                     0                       0  positive  939340800   \n",
       "\n",
       "                          Summary  \\\n",
       "138706  EVERY book is educational   \n",
       "\n",
       "                                                     Text  \\\n",
       "138706  this witty little book makes my son laugh at l...   \n",
       "\n",
       "                                            cleanned_text  \n",
       "138706  b'witti littl book make son laugh loud recit c...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store final table into SQLLite table for future\n",
    "conn=sqlite3.connect('final.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory=str\n",
    "final_data.to_sql('Reviews',conn,schema=None,if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams, Bigrams and n Grams:\n",
    "#### Unigrams:\n",
    "    divide the whole sentence into 1 dimension words. Or in other words each word is considered a dimension. But in this case we may loose sequential information.\n",
    "#### Bi-grams:\n",
    "    Pairs of words is considered as a dimension.\n",
    "#### Tri-grams:\n",
    "    3- consecutive words are considered as a dimension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_pos=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_neg=nltk.FreqDist(all_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 common positive words :  [(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791)]\n",
      "\n",
      "Top 10 common negative words:  [(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 common positive words : ',freq_dist_pos.most_common(10))\n",
    "print('\\nTop 10 common negative words: ',freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bigram\n",
    "bigram_obj=CountVectorizer(ngram_range=(2,2))\n",
    "bigram_count=bigram_obj.fit_transform(final_data['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2794911)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count.get_shape()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf (Term frequency and Inverse Document Frequency)\n",
    "#### Term Frequency:\n",
    "    The probability of finding a word in a given document\n",
    "#### Inverse Document Frequency: \n",
    "\n",
    "    log(N/ni)\n",
    "* N = total number of documents in corpus\n",
    "* ni = total number of documents in which the word present\n",
    "* (N/ni)>1 \n",
    "* and log(1)=0\n",
    "* the value of idf will always >=1\n",
    "\n",
    "The log of the total numbers of documents in the corpus devided by number of document that contains the word. So this value will always greater than 0.\n",
    "\n",
    "\n",
    "* if the word is more frequent then idf will be low\n",
    "* and if the word is rare frequent then idf will be high\n",
    "\n",
    "So in Tf-idf we multiply the value of tf and idf for each word and put that value corospond to the vector value in each documents sparse vector.\n",
    "\n",
    "The only disadvantage of the tf-idf is that it not takes the sementic meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_obj=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf=tf_idf_obj.fit_transform(final_data['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 2910192)\n",
      "2910192\n"
     ]
    }
   ],
   "source": [
    "print(final_tf_idf.get_shape())\n",
    "feature_names=tf_idf_obj.get_feature_names()\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ales until', 'ales ve', 'ales would', 'ales you', 'alessandra', 'alessandra ambrosia', 'alessi', 'alessi added', 'alessi also', 'alessi and']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[100000:100010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get top tf_idf values:\n",
    "#source : https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "def top_tf_idf_feat(raw,features,top_n):\n",
    "    topn_ids=np.argsort(raw)[::-1][:top_n]\n",
    "    #print(topn_ids)\n",
    "    #type(topn_ids)\n",
    "    top_feats = [(features[i], raw[i]) for i in topn_ids]\n",
    "    df=pd.DataFrame(top_feats)\n",
    "    df.columns=['Features','if_idf_score']\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Features  if_idf_score\n",
      "0       sendak books      0.173437\n",
      "1        rosie movie      0.173437\n",
      "2    paperbacks seem      0.173437\n",
      "3      cover version      0.173437\n",
      "4       these sendak      0.173437\n",
      "5     the paperbacks      0.173437\n",
      "6         pages open      0.173437\n",
      "7       really rosie      0.168074\n",
      "8  incorporates them      0.168074\n",
      "9         paperbacks      0.168074\n"
     ]
    }
   ],
   "source": [
    "row=final_tf_idf[1,:].toarray()[0]\n",
    "top_tf_idf=top_tf_idf_feat(row,feature_names,10)\n",
    "print(top_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "It preserves the sementic meaning as well as their relationships (actual meaning like men and women vs kind and queen). It didn't store terms as sparse matrix.If W1 and W2 are semantically similiar then its vector v1 and v2 will be closed. (Vman-Vwoman) is parallel to (Vkind-Vqueen). W2V is learning the semantic automatically by raw text. The larger the dimension tends to more more information in W2V algorithm. This is really very powerfull Technique\n",
    "* The larger dimension tends to more information rich vector we have. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "i=0\n",
    "list_of_sentences=[]\n",
    "for sent in final_data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=clean_tags(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in clean_panctuations(w).split():\n",
    "            if (cleaned_words.isalpha()):\n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue\n",
    "    list_of_sentences.append(filtered_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.Word2Vec(list_of_sentences,min_count=5,size=50,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this', 'witty', 'little', 'book', 'makes']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=list(w2v_model.wv.vocab)\n",
    "print(len(words))\n",
    "#w2v_model.wv.most_similar('tasty')\n",
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resemble', 0.7106888890266418),\n",
       " ('dislike', 0.647260844707489),\n",
       " ('mean', 0.643530547618866),\n",
       " ('prefer', 0.6375082731246948),\n",
       " ('think', 0.6203250885009766),\n",
       " ('overpower', 0.6108693480491638),\n",
       " ('overwhelm', 0.6095193028450012),\n",
       " ('enjoy', 0.6023695468902588),\n",
       " ('bother', 0.5895633697509766),\n",
       " ('miss', 0.5820777416229248)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Word2Vec:\n",
    "    Average Word2Vec is the average of all the Word2Vec for each word in a given document. It is used to Calculate Word2Vec for sentences\n",
    "                 \n",
    "                 Average Word2Vec = 1/n(Word2Vec(w1)+Word2Vec(W2)+........)\n",
    "#### Tf-idf-Word2Vec:\n",
    "    Simple weighting stretegies to convert sentences into vecFirst it compute Tf-idf for document r1 and return a vector for document r1. And simulteneuosly calculate Word2Vec for each of the word and multiply with that document vector's word index.\n",
    "        \n",
    "      Tf-idf-Word2Vec(w1) = (t1*Word2Vec(w1)+t2*Word2Vec(w2)+...)/t1+t2+... , Where ti is the tfidf value of wi word\n",
    "      Tf-idf-Word2Vec(sentence)= (ti*Word2Vec(wi))/(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'grew', 'up', 'reading', 'these', 'sendak', 'books', 'and', 'watching', 'the', 'really', 'rosie', 'movie', 'that', 'incorporates', 'them', 'and', 'love', 'them', 'my', 'son', 'loves', 'them', 'too', 'i', 'do', 'however', 'miss', 'the', 'hard', 'cover', 'version', 'the', 'paperbacks', 'seem', 'kind', 'of', 'flimsy', 'and', 'it', 'takes', 'two', 'hands', 'to', 'keep', 'the', 'pages', 'open']\n"
     ]
    }
   ],
   "source": [
    "print((list_of_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yeah', 0.7761039733886719),\n",
       " ('lol', 0.7219513654708862),\n",
       " ('uh', 0.7139227390289307),\n",
       " ('stupid', 0.7081554532051086),\n",
       " ('seriously', 0.6896477937698364),\n",
       " ('heck', 0.6497436761856079),\n",
       " ('kidding', 0.6401499509811401),\n",
       " ('yep', 0.6397970914840698),\n",
       " ('damn', 0.6382590532302856),\n",
       " ('anyways', 0.6269793510437012)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('hey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Word2Vec for each sentences in review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n"
     ]
    }
   ],
   "source": [
    "sent_vector=[]\n",
    "for sent in list_of_sentences:\n",
    "    sent_vec=np.zeros(50)\n",
    "    count_words=0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.wv[word]\n",
    "            sent_vec+=vec\n",
    "            count_words+=1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec=sent_vec/count_words\n",
    "    sent_vector.append(sent_vec)\n",
    "\n",
    "print(len(sent_vector))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'grew', 'up', 'reading', 'these', 'sendak', 'books', 'and', 'watching', 'the', 'really', 'rosie', 'movie', 'that', 'incorporates', 'them', 'and', 'love', 'them', 'my', 'son', 'loves', 'them', 'too', 'i', 'do', 'however', 'miss', 'the', 'hard', 'cover', 'version', 'the', 'paperbacks', 'seem', 'kind', 'of', 'flimsy', 'and', 'it', 'takes', 'two', 'hands', 'to', 'keep', 'the', 'pages', 'open']\n",
      "[-0.30392123 -0.79201868  0.33612066 -0.35314329 -0.52123807 -0.13319498\n",
      " -0.60553056 -0.69415923 -0.05810921 -0.19672782 -0.68593011  0.8393152\n",
      " -0.43446464 -0.24793821  0.61800498 -0.0364209   0.05479705  1.14821459\n",
      " -0.65693565 -0.30334259 -1.00440464 -0.0327446  -0.2563789   1.31766732\n",
      " -0.37717317 -0.37804507  1.33098585 -0.44040332  0.34808814 -0.4387633\n",
      "  0.31092072  0.16986976 -0.44022827  0.09674198  0.12834727  0.76208363\n",
      "  0.27461769 -0.12432189  0.3808894   1.20819422 -0.14434112 -0.00211157\n",
      "  0.42197797 -1.00044272 -0.63553785  1.08340417 -0.47544294 -0.06401876\n",
      " -0.73018969  0.25860468]\n"
     ]
    }
   ],
   "source": [
    "print((list_of_sentences[1]))\n",
    "print((sent_vector[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going towards tf-idf-word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features=tf_idf_obj.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910192\n"
     ]
    }
   ],
   "source": [
    "print(len(tf_idf_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1268863)\t0.08275523980718573\n",
      "  (0, 1322643)\t0.05736320423975409\n",
      "  (0, 1181493)\t0.05904930622559946\n",
      "  (0, 2815806)\t0.06353114992572337\n",
      "  (0, 1562605)\t0.12941275579745923\n",
      "  (0, 1032033)\t0.11573055882069516\n",
      "  (0, 2075535)\t0.12941275579745923\n",
      "  (0, 2616462)\t0.12541096971801555\n",
      "  (0, 49126)\t0.04475667962178877\n",
      "  (0, 283308)\t0.054444217381069276\n",
      "  (0, 2381603)\t0.0769491583240197\n",
      "  (0, 2837880)\t0.07781013221409352\n",
      "  (0, 2324657)\t0.0937743722525583\n",
      "  (0, 324043)\t0.11573055882069516\n",
      "  (0, 2608442)\t0.10301992784348538\n",
      "  (0, 2838349)\t0.06715422842681543\n",
      "  (0, 126268)\t0.0893377308667213\n",
      "  (0, 361978)\t0.11856987122963351\n",
      "  (0, 552188)\t0.12941275579745923\n",
      "  (0, 1319489)\t0.0927612148772383\n",
      "  (0, 2578785)\t0.026787698091940104\n",
      "  (0, 105321)\t0.06840813514421003\n",
      "  (0, 1333169)\t0.053575946913953454\n",
      "  (0, 1739789)\t0.046544793491389236\n",
      "  (0, 2255427)\t0.12941275579745923\n",
      "  :\t:\n",
      "  (364170, 1254867)\t0.03283894855017248\n",
      "  (364170, 2614666)\t0.05504239102854844\n",
      "  (364170, 2897041)\t0.040848095932348116\n",
      "  (364170, 2550221)\t0.045449354719254265\n",
      "  (364170, 2771454)\t0.02772432091825306\n",
      "  (364170, 2745180)\t0.031851743745175284\n",
      "  (364170, 1169043)\t0.025105274169707193\n",
      "  (364170, 1384531)\t0.05960097701505191\n",
      "  (364170, 2312048)\t0.0378824455359619\n",
      "  (364170, 1063741)\t0.03709433849076868\n",
      "  (364170, 2891706)\t0.02700318583120386\n",
      "  (364170, 2592197)\t0.05406095709752581\n",
      "  (364170, 2843637)\t0.024866018323416738\n",
      "  (364170, 1091864)\t0.029888256751750948\n",
      "  (364170, 2545801)\t0.06524339220457043\n",
      "  (364170, 1647825)\t0.06902525115271677\n",
      "  (364170, 577030)\t0.08919740961914537\n",
      "  (364170, 2606992)\t0.10637648698033227\n",
      "  (364170, 1180506)\t0.1515904332736846\n",
      "  (364170, 139736)\t0.03135077437796279\n",
      "  (364170, 1332766)\t0.03625919264341876\n",
      "  (364170, 218560)\t0.031370049645728135\n",
      "  (364170, 2323768)\t0.0650989545513513\n",
      "  (364170, 1639344)\t0.022562482801020776\n",
      "  (364170, 2574871)\t0.037066476485010325\n"
     ]
    }
   ],
   "source": [
    "print(final_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n"
     ]
    }
   ],
   "source": [
    "tf_idf_w2v_sent_vector=[]\n",
    "row=0\n",
    "for sent in list_of_sentences:\n",
    "    sent_vect=np.zeros(50)\n",
    "    tf_idf_sum=0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.wv[word]\n",
    "            tf_idf_word=final_tf_idf[row,tf_idf_feature.index(word)]\n",
    "            sent_vect+=(vec*tf_idf_word)\n",
    "            tf_idf_sum+=tf_idf_word\n",
    "        except:\n",
    "            pass\n",
    "    sent_vect=sent_vect/tf_idf_sum\n",
    "    tf_idf_w2v_sent_vector.append(sent_vec)\n",
    "    row+=1\n",
    "\n",
    "print(len(tf_idf_w2v_sent_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.26661966, -0.03295791, -0.25479811, -0.46635248,  0.34355303,\n",
      "       -0.39239626, -0.68777127, -0.88658103,  0.1324354 ,  0.07070138,\n",
      "       -0.35070344,  0.81264022, -0.53939053, -0.09726452,  0.57552282,\n",
      "       -0.41481607,  0.47379984,  1.75258809, -0.46194684,  0.59258366,\n",
      "       -0.78632025,  0.34842148, -0.28700296,  0.86899327, -0.50096115,\n",
      "       -0.19244379,  1.24692652, -0.7663961 ,  0.62684289,  0.26949071,\n",
      "        0.0308574 ,  0.43166185,  0.10855806, -0.40827156,  0.45064414,\n",
      "        1.13380192,  0.35173921,  0.78460497,  0.55599871,  1.46632871,\n",
      "       -2.08668473, -0.10661922,  0.50243684, -0.49826203, -0.88207639,\n",
      "        0.78687283, -1.03372064,  0.17053629, -0.45100903,  1.34717095]), array([-0.26661966, -0.03295791, -0.25479811, -0.46635248,  0.34355303,\n",
      "       -0.39239626, -0.68777127, -0.88658103,  0.1324354 ,  0.07070138,\n",
      "       -0.35070344,  0.81264022, -0.53939053, -0.09726452,  0.57552282,\n",
      "       -0.41481607,  0.47379984,  1.75258809, -0.46194684,  0.59258366,\n",
      "       -0.78632025,  0.34842148, -0.28700296,  0.86899327, -0.50096115,\n",
      "       -0.19244379,  1.24692652, -0.7663961 ,  0.62684289,  0.26949071,\n",
      "        0.0308574 ,  0.43166185,  0.10855806, -0.40827156,  0.45064414,\n",
      "        1.13380192,  0.35173921,  0.78460497,  0.55599871,  1.46632871,\n",
      "       -2.08668473, -0.10661922,  0.50243684, -0.49826203, -0.88207639,\n",
      "        0.78687283, -1.03372064,  0.17053629, -0.45100903,  1.34717095])]\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_w2v_sent_vector[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
